{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 1: Basic Text Processing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lines:  4689\n"
     ]
    }
   ],
   "source": [
    "with open(\"三体3死神永生-刘慈欣.txt\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "print('# of lines: ', len(raw))\n",
    "raw = ''.join(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T0. Cleaning the raw data\n",
    "\n",
    "1. Replace the special token `\\u3000` with empty string \"\".\n",
    "2. Replace consecutive newlines with just a single one.\n",
    "3. Other cleaning work you can think of.\n",
    "\n",
    "*Hint*: Use `re.sub()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9307\n",
      "before replacing: the '　' number is 9307\n",
      "after replacing the amount of '　' number is 0\n",
      "before replacing the consecutive newlines, the newlines number is 4688\n",
      "after replacing, the number of newline is 4654\n"
     ]
    }
   ],
   "source": [
    "def count_pattern(string, pattern):\n",
    "    matches = re.findall(pattern, string)\n",
    "    return len(matches)\n",
    "\n",
    "cnt0 = raw.count('\\u3000')\n",
    "print(cnt0)\n",
    "print(\"before replacing: the '\\u3000' number is \" + str(cnt0))\n",
    "no_3000_raw = re.sub('\\u3000', \"\", raw)\n",
    "cnt1 = no_3000_raw.count('\\u3000')\n",
    "print(\"after replacing the amount of '\\u3000' number is \" + str(cnt1))\n",
    "\n",
    "pattern = r\"[\\n]\"\n",
    "newline_cnt = count_pattern(no_3000_raw, pattern)\n",
    "\n",
    "print('before replacing the consecutive newlines, the newlines number is ' + str(newline_cnt))\n",
    "no_consecutive_newline_raw = re.sub('[\\n]+', '\\n', no_3000_raw)\n",
    "newline_cnt = count_pattern(no_consecutive_newline_raw, pattern)\n",
    "\n",
    "print('after replacing, the number of newline is ' + str(newline_cnt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Count the number of Chinese tokens\n",
    "\n",
    "*Hint*: Use `re.findall()` and the range of Chinese characters in Unicode, i.e., `[\\u4e00-\\u9fa5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329946\n"
     ]
    }
   ],
   "source": [
    "ran = re.findall('[\\u4e00-\\u9fa5]', raw)\n",
    "print(len(ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Build the vocabulary for all Chinese tokens\n",
    "\n",
    "Use a Python `dict` object or instance of  `collections.Counter()` to count the frequency of each Chinese token.\n",
    "\n",
    "*Hint*: Go through the `raw` string and for each unique Chinese token, add it to the `dict` or `Counter` object with a count of 1. If the token is already in the `dict` or `Counter` object, increment its count by 1.\n",
    "\n",
    "Check the vocabulary size and print the top 20 most frequent Chinese tokens and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique Chinese character is3027\n",
      "[('的', 15990), ('一', 6749), ('是', 4837), ('在', 4748), ('了', 4149), ('有', 3656), ('这', 3532), ('个', 3458), ('不', 3117), ('人', 2988), ('中', 2649), ('到', 2632), ('他', 2354), ('上', 2194), ('们', 2164), ('时', 2076), ('心', 2007), ('地', 1953), ('大', 1938), ('来', 1855)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "vocab = collections.Counter()\n",
    "for i in ran:\n",
    "    if i not in vocab:\n",
    "        vocab[i] = 1\n",
    "    else:\n",
    "        vocab[i] += 1\n",
    "sorted_counters = vocab.most_common()\n",
    "print(\"The number of unique Chinese character is\" + str(len(vocab)))\n",
    "print(sorted_counters[:20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Sentence segmentation\n",
    "\n",
    "Estimate the number of sentences in the `raw` string by separating the sentences with the delimiter punctuations, such as  `。`, `？`, `！` etc.\n",
    "\n",
    "*Hint*: Use `re.split()` and the correct regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9692\n"
     ]
    }
   ],
   "source": [
    "num_line = len(re.split(r\"[\\u3002|\\uff1f|\\uff01]\", raw))\n",
    "print(num_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences obtained with `re.split()` do not contain the delimiter punctuations. What if we want to keep the delimiter punctuations in the sentences?\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9611\n"
     ]
    }
   ],
   "source": [
    "cnt = len(re.findall(r\"[^\\u3002|\\uff1f|\\uff01]+[。？！]\", raw))\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Count consecutive English and number tokens\n",
    "\n",
    "Estimate the number of consecutive English and number tokens in the `raw` string. Build a vocabulary for them and count their frequency.\n",
    "\n",
    "*Hint*: Use `re.findall()` and the correct regular expression. Use similar method as in T2 to build the vocabulary and count the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201X', '2208', '2208', '2270', '2270', '2272', '2272', '2332', '2333', '2400', '2273', 'DX3906', '2687', '18906416', '647', 'l8906416', '1453', '537', '1204', '1453', '16', '1453', '28', '21', '29', 'ETO', '12', '479', '100', '1000', 'PDC', '100', '286', '286', '286', 'DX3906', 'DX3906', 'DX3906', 'DX3906', 'DX3906', 'DX3906', 'PDC', 'PIA', 'PIA', '18', 'PIA', 'PIA', '18', 'PIA', 'PIA', 'PIA', 'PIA', 'PIA', 'PIA', 'PIA', 'PIA', 'PIA', 'PDC', 'NASA', 'PIA', 'PDC', 'MD', 'MD', 'MD', 'MD', 'MD', 'PIA', 'PIA', 'PIA', 'PIA', 'PDC', 'MD', 'PDC', 'PDC', 'PIA', 'PDC', 'PIA', 'PDC', 'PDC', 'NASA', 'PDC', 'PIA', 'PIA', 'PDC', 'PIA', 'PDC', 'PDC', 'PDC', 'DX3906', 'CIA', 'DX3906', 'DX3906', 'DX3906', 'PIA', 'DX3906', 'PIA', 'PIA', 'PIA', 'PDC', 'PIA', 'PIA', 'PIA', 'PDC', 'PIA', 'NASA', '18', '18', '18', '18', 'PIA', 'PIA', 'PIA', '12', '208', '11', '26', '13', '45', '1775', '59', '59', '1847', '1670', '94', '50', '31', '1967', '36', '1768', '138', 'NH558J2', '61', '300', 'DX3906', 'DX3906', 'AA', 'AA', 'DX3906', 'DX3906', 'AA', 'AA', 'AA', 'DX3906', 'AA', 'DX3906', 'AA', 'AA', '20', '80', '21', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '80', '91', '98', '20', '1974', 'Perimeter', '2009', 'Perimeter', '20', '187J3X1', 'AA', 'AA', 'AA', 'AA', '798', '21', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'PIA', 'PIA', 'AA', 'AA', 'AA', 'DX3906', '62', '436950', '62', '11', '28', '16', '00', '16', '17', '500', 'PDC', 'PDC', 'PDC', 'PDC', 'PDC', '62', '11', '28', '16', '17', '34', '16', '27', '58', '25000', 'PDC', 'PDC', '1400', '1350', '1300', '1200', '1150', '1050', '160', '1000', '900', '900', '750', '750', '600', '21', '18', '3000', '7000', '6000', '4000', '5000', '650', '5500', '3500', '5800', '7000', '6450', '250', '35', '600', '450', '450', '300', '300', '150', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'SIM', 'SIM', 'SIM', 'AA', 'AA', 'AA', 'AA', '21', 'Reservation', 'AA', 'AA', 'IT', '2D', 'AA', 'AA', '21', '20', '20', 'ETO', 'AA', 'AA', '1988', 'AA', 'AA', '62', '11', '28', '16', '17', '34', '16', '27', '58', '1415', '944', '795', '944', '20', '11', '13', '17', '19', '23', '29', '31', '37', '11', '13', '11', '13', '16', '10', '10', '11', '13', '19', '15', '11', '13', '16', '14', '8x8', '56', 'AA', 'AA', '187J3X1', '187J3X1', '187J3X1', 'AA', 'AA', '14', '187J3X1', 'PDC', '19', 'km', 'ETO', 'AA', 'PDC', 'AA', '15', 'PDC', '15G', '1G', 'A225', 'PDC', 'PDC', 'TNT', '3G', 'PDC', 'IDC', 'IDC', 'IDC', 'IDC', 'PIA', 'AA', 'IDC', 'AA', 'AA', 'PIA', 'PIA', 'AA', 'IDC', 'IDC', 'IDC', 'IDC', 'IDC', 'IDC', 'IDC', 'IDC', '187J3X1', 'IDC', 'IDC', 'AA', 'IDC', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'CEO', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'IDC', 'AA', 'AA', 'IDC', 'PIA', 'AA', 'AA', 'IDC', 'AA', 'AA', 'AA', '18', 'IDC', 'IDC', 'IDC', 'IDC', 'AA', 'IDC', 'AA', 'IDC', 'AA', 'AA', 'IDC', 'AA', 'AA', '19', 'AA', 'AA', '21', 'AA', 'AA', '23', '16', '16', '16', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'IDC', 'PDC', 'PDC', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '15', 'AA', '82', '50', '26', 'AA', '82', '50', '26', '10', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'IDC', 'PDC', 'AA', 'AA', 'AA', 'AA', 'AA', '187J3X1', '415', 'PDC', 'TNT', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'Send', 'cerebra', 'only', 'AA', 'AA', 'AA', '11', '37813', '62', '21', '13', '238', '11', '14', '17', '37813', '28', '11', 'Way', '2008', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'Way', 'way', '11', 'AA', '021', 'AA', 'AA', 'AA', 'AA', '67', 'II', '67', 'AA', 'AA', 'AA', '67', '19', 'AA', 'AA', 'PIA', 'PDC', 'AA', '66', 'Ice', 'Ice', 'Ice', 'Ice', '22', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', 'Ice', '187J3X1', 'Ice', '67', 'AA', 'AA', 'AA', '1G', 'AA', 'AA', '1G', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'PIA', 'PIA', 'PPT', '1OG', '1G', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'PIA', 'AA', 'AA', '67', 'AA', '1G', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '1889', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '23', '41', 'AA', 'AA', 'AA', 'AA', '16', 'AA', '16', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '180', 'AA', 'AA', 'AA', 'AA', 'DX3906', 'S74390E2', 'AA', '409', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'DX3906', 'DX3906', 'AA', 'AA', 'AA', 'DX3906', 'AA', 'AA', '4G', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '35', '63', '53', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', 'AA', '5G', '3G', '3G', '3G', '3G', '3G', '3G', 'DX3906', '2G', 'AA', 'AA', '1G', 'DX3906', 'AA', '1K', '68', '43', '297', '52', '170', 'OK', 'DX3906', '10', '6177906', '18903729', 'DX3906', '18903729', 'AA', 'AA', '1G', '647', '647', 'AA', 'AA', '647', '647', '647', '647', 'AA', 'AA', '647', 'AA', '647', '647', '647', '647', '647', 'PIA', '647', '647', '647', '647', '647', '647']\n",
      "Counter({'AA': 338, 'PIA': 45, 'PDC': 35, 'Ice': 34, 'IDC': 28, 'DX3906': 27, 'Way': 20, '647': 19, '16': 14, '11': 13, '21': 8, '18': 8, '187J3X1': 8, '13': 7, '1G': 7, '3G': 7, 'MD': 6, '20': 6, '28': 5, '62': 5, '17': 5, '19': 5, '67': 5, '10': 4, '1453': 3, 'ETO': 3, '286': 3, 'NASA': 3, '26': 3, '50': 3, '300': 3, 'SIM': 3, '23': 3, '15': 3, '14': 3, '2208': 2, '2270': 2, '2272': 2, '29': 2, '12': 2, '100': 2, '1000': 2, '59': 2, '31': 2, '80': 2, 'Perimeter': 2, '34': 2, '27': 2, '58': 2, '900': 2, '750': 2, '600': 2, '7000': 2, '35': 2, '450': 2, '944': 2, 'TNT': 2, '82': 2, '37813': 2, '18903729': 2, '201X': 1, '2332': 1, '2333': 1, '2400': 1, '2273': 1, '2687': 1, '18906416': 1, 'l8906416': 1, '537': 1, '1204': 1, '479': 1, 'CIA': 1, '208': 1, '45': 1, '1775': 1, '1847': 1, '1670': 1, '94': 1, '1967': 1, '36': 1, '1768': 1, '138': 1, 'NH558J2': 1, '61': 1, '91': 1, '98': 1, '1974': 1, '2009': 1, '798': 1, '436950': 1, '00': 1, '500': 1, '25000': 1, '1400': 1, '1350': 1, '1300': 1, '1200': 1, '1150': 1, '1050': 1, '160': 1, '3000': 1, '6000': 1, '4000': 1, '5000': 1, '650': 1, '5500': 1, '3500': 1, '5800': 1, '6450': 1, '250': 1, '150': 1, 'Reservation': 1, 'IT': 1, '2D': 1, '1988': 1, '1415': 1, '795': 1, '37': 1, '8x8': 1, '56': 1, 'km': 1, '15G': 1, 'A225': 1, 'CEO': 1, '415': 1, 'Send': 1, 'cerebra': 1, 'only': 1, '238': 1, '2008': 1, 'way': 1, '021': 1, 'II': 1, '66': 1, '22': 1, 'PPT': 1, '1OG': 1, '1889': 1, '41': 1, '180': 1, 'S74390E2': 1, '409': 1, '4G': 1, '63': 1, '53': 1, '5G': 1, '2G': 1, '1K': 1, '68': 1, '43': 1, '297': 1, '52': 1, '170': 1, 'OK': 1, '6177906': 1})\n"
     ]
    }
   ],
   "source": [
    "box = collections.Counter()\n",
    "ran = re.findall(r\"[A-Za-z0-9]{2,}\", raw)\n",
    "print(ran)\n",
    "for i in ran:\n",
    "    if i not in box:\n",
    "        box[i] = 1\n",
    "    else:\n",
    "        box[i] += 1\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5. Mix of patterns\n",
    "\n",
    "There are two characters whose names are \"艾AA\" and \"程心\". Find all sentences where \"艾AA\" and \"程心\" appear together. Consider fullnames only, that is, \"艾AA\" but not \"AA\" alone. \n",
    "\n",
    "*Hint*: You may find the lookbehind or lookahead pattern useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "　　在程心眼中，艾AA是个像鸟一般轻灵的女孩子，充满生机地围着她飞来飞去。她自称熟悉公元人，因为自己的导师就是一位公元世纪的物理学家。也许是这个原因，她得到了毕业后的第一份工作，被指定为程心与联合国太空开发署之间的联络人\n",
      "\n",
      "　　程心听到有人叫自己的名字，转身一看，竟是艾AA正向这里跑过来。她穿着白色的风衣，长发被风吹起，喘着气说她来看程心，但他们不让她进去\n",
      "程心让艾AA在原地等着自己，但AA坚持要随程心去，只好让她上了车\n",
      "\n",
      "　　程心和艾AA是随最早的一批移民来到澳大利亚的。程心本来可以去堪培拉或悉尼这样的大城市过比较舒适的生活，但她坚持做一个普通移民，来到内陆条件最差的、位于沃伯顿附近沙漠中的移民区\n",
      "\n",
      "　　#第三部\n",
      "　　【广播纪元7年，程心】\n",
      "　　艾AA说程心的眼睛比以前更明亮更美丽了，也许她没有说谎\n",
      "”坐在程心旁边的艾AA大叫起来，引来众人不满的侧目。她不是IDC的委员，是作为程心的顾问和助理参加会议的，这也是由于程心的坚持\n",
      "\n",
      "　　这天，艾AA来找程心\n",
      "\n",
      "　　是艾AA建议程心报名参加试验的，她认为这是为星环公司参与掩体工程而树立公众形象的一次极佳的免费广告，同时，她和程心都清楚试验是经过严密策划的，只是看上去刺激，基本没什么危险\n",
      "\n",
      "　　程心现在身处的世界是一个白色的球形空间，她看到艾AA飘浮在附近，和她一样身穿冬眠时的紧身服，头发湿漉漉的，四肢无力地摊开，显然也是刚刚醒来。她们目光相遇时，程心想说话，但低温造成的麻痹还没有过去，她发不出声来\n",
      "\n",
      "　　在这段时间里，艾AA总是在一旁含情脉脉地看着关一帆。这一天，他们之间的关系有了微妙的进展。在旅行中，AA总是设法与关一帆接近，当后者说话时，她总是全神贯注地倾听，还不时地微笑点头。以前，她从未在任何男人面前有过这种表现。在与程心结识后的这几个世纪，AA有过无数的情人，而且经常同时有两个以上——这是新时代正常的生活状态，但程心知道，AA从来没有真正爱过一个男性\n",
      "\n",
      "　　“艾AA知道把字刻在石头上。”程心像在自语\n",
      "”\n",
      "　　“云天明和艾AA为什么不到这里来呢？”关一帆问。这也是程心最想知道的，她之所以还没问，是怕得到一个悲哀的答案\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(?<=[\\u3002|\\uff1f|\\uff01])([^\\u3002\\uff1f\\uff01]*?(?:艾AA).*?(?:程心)[^\\u3002\\uff1f\\uff01]*?)(?=\\u3002|\\uff1f|\\uff01)\"\n",
    "sentences = re.findall(pattern, raw)\n",
    "for i in sentences:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
